{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = layers.conv2D(32, (8, 8), strides=(4,4), activation=\"relu\",input_shape=(state_size[0], state_size[1], 1))\n",
    "        self.fc1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.fc2 = layers.Dense(action_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        return nn.functional.softmax(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # this is the neural net for DQN\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # this is the neural net for DQN\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        target_model = self._build_model()\n",
    "        target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "\n",
    "                best_next_action = np.argmax(self.model.predict(next_state)[0])\n",
    "\n",
    "                target = (reward + self.gamma * target_model.predict(next_state)[0][best_next_action])\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PER with DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transision', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.priorities = np.zeros(2 * capacity - 1)\n",
    "        self.write_idx = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.priorities[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.priorities):\n",
    "            return idx\n",
    "        \n",
    "        if s <= self.priorities[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.priorities[left])\n",
    "        \n",
    "    def total(self):\n",
    "        return self.priorities[0]  # root node is the total priority\n",
    "    \n",
    "    def add(self, priority , data):\n",
    "        idx = self.write_idx + self.capacity - 1\n",
    "        \n",
    "        self.data[self.write_idx] = data\n",
    "        self._propagate(idx, priority)\n",
    "\n",
    "        self.write_idx += 1\n",
    "        if self.write_idx >= self.capacity:\n",
    "            self.write_idx = 0\n",
    "\n",
    "    \n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        data_idx = idx - self.capacity + 1\n",
    "        return (idx, self.priorities[idx], self.data[data_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PER:\n",
    "    def __init__(self, capacity, alpha, beta, beta_increment):\n",
    "        self.buffer = []\n",
    "        self.priorities = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.priorities.total() / batch_size\n",
    "        is_weights = np.zeros((batch_size, 1))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, priority, data) = self.priorities.get(s)\n",
    "            is_weights[i, 0] = (self.priorities.total() * priority) ** (-self.beta)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "        \n",
    "        is_wights /= is_weights.max()\n",
    "        return batch, idx, is_weights\n",
    "    \n",
    "    def update_priorities(self, idxs, priorities):\n",
    "        for idx, priority in zip(idxs, priorities):\n",
    "            self.priorities._propagate(idx, priority - self.priorities[idx])  # update the priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "\n",
    "from zmq import device\n",
    "\n",
    "\n",
    "class DDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.linear(128, self.output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        QValues = self.layers(state)\n",
    "        return QValues\n",
    "    \n",
    "    def actor(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                q_values = self(state)\n",
    "                action = q_values.max(1)[1].item()  # this is basically the greedy action\n",
    "\n",
    "        else: \n",
    "            action = random.randrange(self.output_dim)  # if the epsilon value not higher then take random actions\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# input and output dim must be set dynamically somehow\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "memory = PER(capacity=10000, alpha=0.6)\n",
    "policy_net = DDQN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "target_net = DDQN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "def update_model():\n",
    "    # sample a batch with priorities\n",
    "    batch, idxs, is_weights = memory.sample(batch_size)\n",
    "\n",
    "    # Convert to tensors, move to device\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.bool).to(device)\n",
    "\n",
    "\n",
    "    current_q_value = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Calculate TD-Target\n",
    "    # (Get next_state Q-values from both current and target networks for DDQN)\n",
    "    with torch.no_grad():\n",
    "        next_states_q_values = policy_net(next_states)\n",
    "        best_next_actions = next_states_q_values.max(1)[1].unsqueeze(1)  # DDQN: Select actions according to policy_net\n",
    "\n",
    "        next_states_target_q_values = target_net(next_states).gather(1, best_next_actions)  # DDQN: Evaluate with target_net\n",
    "\n",
    "        td_target = rewards + (gamma * next_states_target_q_values * (1 - dones))\n",
    "\n",
    "\n",
    "    # calculate the loss using importance sampling weights\n",
    "    loss = (td_target - current_q_value) ** 2 * torch.tensor(is_weights).to(device)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update the priorities in sum-tree\n",
    "    new_priorities = \n",
    "    memory.update_priorities(idxs, new_priorities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
