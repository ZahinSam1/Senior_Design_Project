{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transision', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.priorities = np.zeros(2 * capacity - 1)\n",
    "        self.write_idx = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.priorities[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.priorities):\n",
    "            return idx\n",
    "        \n",
    "        if s <= self.priorities[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.priorities[left])\n",
    "        \n",
    "    def total(self):\n",
    "        return self.priorities[0]  # root node is the total priority\n",
    "    \n",
    "    def add(self, priority , data):\n",
    "        idx = self.write_idx + self.capacity - 1\n",
    "        \n",
    "        self.data[self.write_idx] = data\n",
    "        self._propagate(idx, priority)\n",
    "\n",
    "        self.write_idx += 1\n",
    "        if self.write_idx >= self.capacity:\n",
    "            self.write_idx = 0\n",
    "\n",
    "    \n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        data_idx = idx - self.capacity + 1\n",
    "        return (idx, self.priorities[idx], self.data[data_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PER:\n",
    "    def __init__(self, capacity, alpha, beta, beta_increment):\n",
    "        self.buffer = []\n",
    "        self.priorities = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.add(self.max_priority, experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.priorities.total() / batch_size\n",
    "        is_weights = np.zeros((batch_size, 1))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "\n",
    "            (idx, priority, data) = self.priorities.get(s)\n",
    "            \n",
    "            is_weights[i, 0] = (self.priorities.total() * priority) ** (-self.beta)  # imp_samp weights = ((1/N) * 1/prio(i))^beta\n",
    "            \n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "        \n",
    "        is_wights /= is_weights.max()\n",
    "        \n",
    "        return batch, idx, is_weights\n",
    "    \n",
    "    def update_priorities(self, idxs, priorities):\n",
    "        for idx, priority in zip(idxs, priorities):\n",
    "            self.priorities._propagate(idx, priority - self.priorities[idx])  # update the priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.linear(128, self.output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        QValues = self.layers(state)\n",
    "        return QValues\n",
    "    \n",
    "    def actor(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                q_values = self(state)\n",
    "                action = q_values.max(1)[1].item()  # this is basically the greedy action\n",
    "\n",
    "        else: \n",
    "            action = random.randrange(self.output_dim)  # if the epsilon value not higher then take random actions\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3108991969.py, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 51\u001b[1;36m\u001b[0m\n\u001b[1;33m    new_priorities =\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# input and output dim must be set dynamically somehow\n",
    "\n",
    "\n",
    "memory = PER(capacity=10000, alpha=0.6)\n",
    "policy_net = DDQN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "target_net = DDQN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "def update_model():\n",
    "    # sample a batch with priorities\n",
    "    batch, idxs, is_weights = memory.sample(batch_size)\n",
    "\n",
    "    # Convert to tensors, move to device\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.bool).to(device)\n",
    "\n",
    "\n",
    "    current_q_value = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Calculate TD-Target\n",
    "    # (Get next_state Q-values from both current and target networks for DDQN)\n",
    "    with torch.no_grad():\n",
    "        next_states_q_values = policy_net(next_states)\n",
    "        best_next_actions = next_states_q_values.max(1)[1].unsqueeze(1)  # DDQN: Select actions according to policy_net\n",
    "\n",
    "        next_states_target_q_values = target_net(next_states).gather(1, best_next_actions)  # DDQN: Evaluate with target_net\n",
    "\n",
    "        td_target = rewards + (gamma * next_states_target_q_values * (1 - dones))\n",
    "\n",
    "\n",
    "    # calculate the loss using importance sampling weights\n",
    "    loss = (td_target - current_q_value) ** 2 * torch.tensor(is_weights).to(device)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update the priorities in sum-tree\n",
    "    new_priorities = np.abs(td_target - current_q_value).detach().cpu().numpy()  #calculate absolute TD-Error\n",
    "    # We take the absolute values as priorities should represent the magnitude of the errors.\n",
    "    \n",
    "    memory.update_priorities(idxs, new_priorities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
